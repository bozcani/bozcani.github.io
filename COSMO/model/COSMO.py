import tensorflow as tf
import numpy as np
import os
import pandas
from datetime import datetime

#uyarilari kapatmak icin
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

from os import listdir
from os.path import isfile, join
from random import shuffle

import zconfig

import utils, utils2



class COSMOBM(object):

    """ COSMO implementation using TensorFlow.
    The interface of the class is sklearn-like.
    """

    def __init__(self, num_concepts, num_relations, num_hidden, main_dir='COSMO', model_name='COSMO_model',
                 gibbs_sampling_steps=1, learning_rate=0.01, batch_size=10, num_epochs=1000, verbose=0, save_per_epoch=10, decay_rate=1.):

        """
        :param num_visible: number of visible units
        :param num_hidden: number of hidden units
        :param visible_unit_type: type of the visible units (binary or gaussian)
        :param main_dir: main directory to put the models, data and summary directories
        :param model_name: name of the model, used to save data
        :param gibbs_sampling_steps: optional, default 1
        :param learning_rate: optional, default 0.01
        :param batch_size: optional, default 10
        :param num_epochs: optional, default 10
        :param stddev: optional, default 0.1. Ignored if visible_unit_type is not 'gauss'
        :param verbose: level of verbosity. optional, default 0
        """

        self.num_concepts = num_concepts
        self.num_relations = num_relations
        self.num_hidden = num_hidden
        self.main_dir = main_dir
        self.model_name = model_name
        self.gibbs_sampling_steps = gibbs_sampling_steps
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.verbose = verbose
        self.save_per_epoch = save_per_epoch
        self.decay_rate = decay_rate
        
        self.models_dir, self.data_dir, self.summary_dir = self._create_data_directories()
        self.model_path = self.models_dir + self.model_name

        # Weights
        self.W_CH = None
        self.W_RH = None
        self.L    = None

        self.b_h = None
        self.b_c = None
        self.b_r = None


        self.W_CH_upd8 = None
        self.W_RH_upd8 = None
        self.L_upd8    = None

        self.b_h_upd8 = None
        self.b_c_upd8 = None
        self.b_r_upd8 = None

        self.encode = None

        self.loss = None

        self.input_data = None

        # Random values to compare activation probabilities in sampling
        self.hrand = None
        self.crand = None
        self.rrand = None

        self.validation_size = None

        self.tf_merged_summaries = None
        self.tf_summary_writer = None
        self.tf_session = None
        self.tf_saver = None
        
        #For momentum update
        self.update = None
        self.train_history = []
        self.validation_history = []
        self.temperature = 1.0
    def _create_data_directories(self):

        """ Create the three directories for storing respectively the models,
        the data generated by training and the TensorFlow's summaries.
        :return: tuple of strings(models_dir, data_dir, summary_dir)
        """

        self.main_dir = self.main_dir + '/' if self.main_dir[-1] != '/' else self.main_dir

        models_dir = zconfig.models_dir + self.main_dir
        data_dir = zconfig.data_dir + self.main_dir
        summary_dir = zconfig.summary_dir + self.main_dir

        for d in [models_dir, data_dir, summary_dir]:
            if not os.path.isdir(d):
                os.mkdir(d)

        return models_dir, data_dir, summary_dir



    def _create_placeholders(self):

        """ Create the TensorFlow placeholders for the model.
        :return: tuple(input(shape(None, num_concept + num_relation*num_concept*num_concept)),
                       hrand(shape(None, num_hidden+1))
                       crand(shape(None, num_concept+1))
		       rrand(shape(None, num_relation, num_concept+1, num_concept+1)))
        """

        x     = tf.placeholder('float', [self.batch_size, self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts], name='x-input')
        hrand = tf.placeholder('float', [self.batch_size, self.num_hidden], name='hrand')
        crand = tf.placeholder('float', [self.batch_size, self.num_concepts], name='crand')
        rrand = tf.placeholder('float', [self.batch_size, self.num_relations, self.num_concepts, self.num_concepts], name='rrand')

        return x, hrand, crand, rrand

    def _create_variables(self):

        """ Create the TensorFlow variables for the model.
        :return: tuple(CH_weights(shape(num_concept+1, num_hidden+1),
                       RH_weights(shape(num_relation, num_concept, num_concept,num_hidden)),
                       L_weights(shape(num_relation, num_concept, num_concept)))
        """

        W_CH = tf.Variable(tf.random_normal((self.num_concepts, self.num_hidden), mean=0.0, stddev=0.01), name='W_CH')
        W_RH = tf.Variable(tf.random_normal((self.num_relations,self.num_concepts,self.num_concepts, self.num_hidden), mean=0.0, stddev=0.01), name='W_RH')
        L    = tf.Variable(tf.random_normal((self.num_relations,self.num_concepts,self.num_concepts), mean=0.0, stddev=0.01), name='L')

        b_h = tf.Variable(tf.zeros([self.num_hidden]))
        b_c = tf.Variable(tf.zeros([self.num_concepts]))
        b_r = tf.Variable(tf.zeros([self.num_relations,self.num_concepts, self.num_concepts]))

        return W_CH, W_RH, L, b_h, b_c, b_r

    def sample_hidden_from_visible(self, cstates,rstates):

        """ Sample the hidden units from the visible units.
        This is the Positive phase of the Contrastive Divergence algorithm.
        :param cstates, rstates: states of the visible units
        :return: tuple(hidden probabilities, hidden binary states)
        """
        # reshape rstates and W_RH for 2D matrix multiplication.
        rstates = tf.reshape(rstates,[-1, self.num_relations*self.num_concepts*self.num_concepts])
        w_rh    = tf.reshape(self.W_RH,[self.num_relations*self.num_concepts*self.num_concepts, self.num_hidden])

        hprobs = tf.nn.sigmoid((tf.matmul(cstates, self.W_CH) + tf.matmul(rstates, w_rh) + self.b_h))
        hstates = utils.sample_prob(hprobs, self.hrand)

        return hprobs, hstates


    def sample_visible_from_hidden(self, hstates, cstates, rstates):

        """ Sample the visible units from the hidden units.
        This is the Negative phase of the Contrastive Divergence algorithm.
        :param hidden: activations of the hidden units, concept units and relation units
        :return: visible probabilities and states.
        """

        c = tf.concat([cstates]*self.num_relations*self.num_concepts,1)
        L_expanded = tf.expand_dims(self.L,0)
        L_expanded = tf.concat([L_expanded]*self.batch_size,0,name='L_expanded')

        # Create a=[r11c1,r12c2,r13,c3,..,r21c1,r22c2,r23,c3,...]
        r = tf.reshape(rstates,[-1, self.num_relations*self.num_concepts*self.num_concepts])
        a = r*c
        a.set_shape((self.batch_size, a.get_shape()[1]))

        # Create a matrix [[L11r11c1,L12r12c2,L13r13c3,..],[L21r21c1,L22r22c2,L23r23c3,..],...]
        a = tf.reshape(a,[self.batch_size, self.num_relations,self.num_concepts,self.num_concepts])
        temp = tf.reduce_sum(a*L_expanded, axis=1)
        s_act  = tf.reduce_sum(temp, axis=2)



        # Create a matrix [[L11r11c1,L12r12c1,L13r13c1,..],[L21r21c2,L22r22c2,L23r23c2,..],...]
        # KONTROL ET, TRANSPOSE ALINCA SORUN COZULDU MU YANI? MUHTEMELEN COZULDU.
        r = tf.transpose(rstates, [0,1,3,2])
        r = tf.reshape(rstates,[-1, self.num_relations*self.num_concepts*self.num_concepts])
        a = r*c
        a.set_shape((self.batch_size, a.get_shape()[1]))
        a = tf.reshape(a,[self.batch_size, self.num_relations,self.num_concepts,self.num_concepts])

        temp = tf.reduce_sum(a*L_expanded, axis=1)
        o_act  = tf.reduce_sum(temp, axis=2)



        def cond(cnt, cunits,cprobs):
            return tf.less(cnt,self.num_concepts)

        def body(cnt, cunits,cprobs):

            c_activation = tf.matmul(hstates, tf.transpose(self.W_CH)) + (s_act + o_act) + self.b_c
            c_probs = tf.nn.sigmoid(c_activation)

            new = utils.sample_prob(c_probs, self.crand)
            new_row = tf.gather(tf.transpose(new), [cnt])


            first_part = tf.gather(tf.transpose(cunits), tf.range(0,cnt) )
            second_part = tf.gather(tf.transpose(cunits), tf.range(cnt+1, self.num_concepts) )

            c_units = tf.transpose(tf.concat([first_part, new_row, second_part], 0))
            c_units.set_shape((self.batch_size,self.num_concepts))


            first_part_prob = tf.gather(tf.transpose(cprobs), tf.range(0,cnt) )
            second_part_prob = tf.gather(tf.transpose(cprobs), tf.range(cnt+1, self.num_concepts) )
            new_row_prob = tf.gather(tf.transpose(c_probs), [cnt])

            c_probs = tf.transpose(tf.concat([first_part_prob, new_row_prob, second_part_prob], 0))
            c_probs.set_shape((self.batch_size,self.num_concepts))

            return cnt+1, c_units,c_probs

        _, c_units, c_probs = tf.while_loop(cond, body, [tf.constant(0), cstates, cstates])


        # TODO relation'larin activation'ini kontrol et.
        w_rh = tf.reshape(self.W_RH,[self.num_relations*self.num_concepts*self.num_concepts, self.num_hidden])
        cc   = 	tf.reshape( tf.concat([c_units]*self.num_concepts,1),[-1,self.num_concepts,self.num_concepts])
        cc_prod = cc*tf.transpose(cc,[0,2,1])
        cc_prod = tf.concat([tf.reshape(cc_prod,[-1,self.num_concepts*self.num_concepts])]*self.num_relations,axis=1,name='cc_prod')
        cc_prod_reshaped = tf.reshape(cc_prod,[-1,self.num_relations, self.num_concepts, self.num_concepts],name='cc_prod_reshaped') #Yanlislik olabilir mi?


        act1 = tf.matmul(hstates,tf.transpose(w_rh))
        act1_reshaped = tf.reshape(act1, [-1,self.num_relations,self.num_concepts,self.num_concepts])
        act2 = cc_prod_reshaped*L_expanded

        r_activation = act1_reshaped+act2 + self.b_r
        r_probs = tf.nn.sigmoid((r_activation))
        r_units = utils.sample_prob(r_probs,self.rrand)

        return c_probs, c_units,r_probs, r_units



    def gibbs_sampling_step(self, concepts, relations):

        """ Performs one step of gibbs sampling.
        :param visible: activations of the visible units
        :return: tuple(hidden probs, hidden states, visible probs(concepts and relations),
                       new hidden probs, new hidden states)
        """

        #concepts = visible[:,0:self.num_concepts]
        #relations = visible[:,self.num_concepts:]
        #relations = tf.reshape(relations, [-1,self.num_relations,self.num_concepts,self.num_concepts] ) #-1 means all

        hprobs, hstates = self.sample_hidden_from_visible(concepts, relations)

        cprobs, cstates, rprobs, rstates = self.sample_visible_from_hidden(hstates, concepts, relations)
        hprobs1, hstates1 = self.sample_hidden_from_visible(cstates, rstates)

        return hprobs, hstates, cprobs, cstates, rprobs, rstates, hprobs1, hstates1




    def _build_model(self):

        """ Build the Restricted Boltzmann Machine model in TensorFlow.
            :return: self
            """
        # ONEMLI NOT pos_assoc_W_RH ve neg_assoc_W_RH teki hstatesler hprob yapildi.
        self.input_data, self.hrand, self.crand, self.rrand = self._create_placeholders()
        self.W_CH, self.W_RH, self.L, self.b_h, self.b_c, self.b_r = self._create_variables()

        #Reshape input data to calculate associations.
        concepts = self.input_data[:,0:self.num_concepts]
        relations = self.input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [-1,self.num_relations,self.num_concepts,self.num_concepts] ) #-1 means all

        hprobs0, hstates0, cprobs, cstates, rprobs, rstates, hprobs1, hstates1 = self.gibbs_sampling_step(concepts,relations)
        
        
        pos_assoc_W_CH = tf.matmul(tf.transpose(concepts), hprobs0, name='pos_assoc_W_CH')

        r = tf.reshape(relations,[-1,self.num_relations*self.num_concepts*self.num_concepts])
        pos_assoc_W_RH = tf.reshape(tf.matmul(tf.transpose(r),hprobs0), [self.num_relations,self.num_concepts,self.num_concepts,self.num_hidden],name='pos_assoc_W_RH')

        cc   = 	tf.reshape( tf.concat([concepts]*self.num_concepts,1),[-1,self.num_concepts,self.num_concepts])
        cc_prod = cc*tf.transpose(cc,[0,2,1])
        cc_prod = tf.concat([tf.reshape(cc_prod,[-1,self.num_concepts*self.num_concepts])]*self.num_relations,axis=1,name='cc_prod')
        cc_prod_reshaped = tf.reshape(cc_prod,[-1,self.num_relations, self.num_concepts, self.num_concepts],name='cc_prod_reshaped') #Yanlislik olabilir mi?
        pos_assoc_L = tf.reduce_sum(cc_prod_reshaped*relations,axis=0,name='pos_assoc_L' )


        nn_input = (cprobs,rprobs)

        for step in range(self.gibbs_sampling_steps - 1):
            hprobs0, hstates0, cprobs, cstates, rprobs, rstates, hprobs1, hstates1 = self.gibbs_sampling_step(concepts,relations)
            nn_input = (cprobs,rprobs)
            
            
            
        neg_assoc_W_CH = tf.matmul(tf.transpose(cprobs), hprobs1, name='neg_assoc_W_CH')
        
        r = tf.reshape(rstates,[-1,self.num_relations*self.num_concepts*self.num_concepts])
        neg_assoc_W_RH = tf.reshape(tf.matmul(tf.transpose(r),hprobs1), [self.num_relations,self.num_concepts,self.num_concepts,self.num_hidden],name='neg_assoc_W_RH')

        # UYARI: kontrol
        cc   = 	tf.reshape( tf.concat([cprobs]*self.num_concepts,1),[-1,self.num_concepts,self.num_concepts])
        cc_prod = cc*tf.transpose(cc,[0,2,1])
        cc_prod = tf.concat([tf.reshape(cc_prod,[-1,self.num_concepts*self.num_concepts])]*self.num_relations,axis=1,name='cc_prod')
        cc_prod_reshaped = tf.reshape(cc_prod,[-1,self.num_relations, self.num_concepts, self.num_concepts],name='cc_prod_reshaped') #Yanlislik olabilir mi?
        neg_assoc_L = tf.reduce_sum(cc_prod_reshaped*rprobs,axis=0,name='neg_assoc_L' )

        #self.encode = hprobs1  # encoded data, used by the transform method

        grad_W_CH = (pos_assoc_W_CH-neg_assoc_W_CH)/self.batch_size
        grad_W_RH = (pos_assoc_W_RH-neg_assoc_W_RH)/self.batch_size
        grad_L = (pos_assoc_L-neg_assoc_L)/self.batch_size
        grad_b_h = tf.reduce_mean(hprobs0 - hprobs1,0)/self.batch_size
        grad_b_c = tf.reduce_mean(concepts - cprobs,0)/self.batch_size
        grad_b_r = tf.reduce_mean(relations- rprobs,0)/self.batch_size
        
        
        # Momentum update
        '''
        momentum = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.9)

        self.update = momentum.apply_gradients([(grad_W_CH,self.W_CH),(grad_W_RH,self.W_RH),(grad_L,self.L),(grad_b_h,self.b_h),(grad_b_c,self.b_c), (grad_b_r,self.b_r)])
        
        '''
        # Vanilla update
        self.W_CH_upd8 = self.W_CH.assign_add(self.learning_rate*(grad_W_CH))
        self.W_RH_upd8 = self.W_RH.assign_add(self.learning_rate*(grad_W_RH))
        self.L_upd8 = self.L.assign_add(self.learning_rate*(grad_L))

        self.b_h_upd8 = self.b_h.assign_add(self.learning_rate * (grad_b_h))
        self.b_c_upd8 = self.b_c.assign_add(self.learning_rate * (grad_b_c))
        self.b_r_upd8 = self.b_r.assign_add(self.learning_rate * (grad_b_r))
        

        #UYARI loss'u degistirmek zorunda kalabilirsin. daha anlasilir bir loss fonksiyonu iyi olabilir. su an direkt relationlarla conceptleri tek bir scalere topluyoruz.
        self.loss_concepts  = (tf.reduce_mean(tf.square(concepts - cprobs)))
        self.loss_relations = (tf.reduce_mean(tf.square(relations[:,:4] - rprobs[:,:4])))
        self.loss_affordances = (tf.reduce_mean(tf.square(relations[:,4:] - rprobs[:,4:])))
        _ = tf.summary.scalar("cost", self.loss_concepts)
        self.acc_concepts  = tf.reduce_mean(tf.cast(tf.equal(concepts, cstates),tf.float64))
        self.acc_relations = tf.reduce_mean(tf.cast(tf.equal(relations[:,:4], rstates[:,:4]),tf.float64))
        self.acc_affordances = tf.reduce_mean(tf.cast(tf.equal(relations[:,4:], rstates[:,4:]),tf.float64))

    def _initialize_tf_utilities_and_ops(self, restore_path=None):

        """ Initialize TensorFlow operations: summaries, init operations, saver, summary_writer.
        Restore a previously trained model if the flag restore_previous_model is true.
        """

        #self.tf_merged_summaries = tf.summary.merge()
        init_op = tf.global_variables_initializer()
        self.tf_saver = tf.train.Saver(max_to_keep=(self.num_epochs/self.save_per_epoch)+1)

        self.tf_session.run(init_op)

        if restore_path!=None:
            self.tf_saver.restore(self.tf_session, restore_path)

        self.tf_summary_writer = tf.summary.FileWriter(self.summary_dir, self.tf_session.graph)

    def _train_model(self, path_to_batches, path_to_validation_set=None):

        """ Train the model.
        :param train_set: training set
        :param validation_set: validation set. optional, default None
        :return: self
        """

        for i in range(self.num_epochs):
            print 'epoch:',i, ' temperature: ', self.temperature
            
            t1 = datetime.now()
            self._run_train_step(path_to_batches)
            t2 = datetime.now()
            
            print 'time(sec): ', (t2-t1).total_seconds()
            
            #decay learning rate in every epoch
            if i%10==0:
                self.learning_rate *= self.decay_rate
            
            if i%self.save_per_epoch==0:
                self.tf_saver.save(self.tf_session, self.model_path+'_'+str(i))

            
	    """
            if i%1==0:
                #self.temperature = 1.0 * ((0.9)**i) # Exponential multiplicative cooling
                #self.temperature = 1.0/(1+0.9*i) # Linear multiplicative cooling
                self.temperature = 1.0/((1+0.9*np.log(1+i))) # Logarithmical multiplicative cooling
            """     
                
            if path_to_validation_set is not None:
                self._run_validation_error_and_summaries(i, path_to_validation_set)


    def _run_train_step(self, path_to_batches):

        """ Run a training step. A training step is made by randomly shuffling the training set,
        divide into batches and run the variable update nodes for each batch.
        :param train_set: training set
        :return: self
        """


        batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]

        shuffle(batches_names)

        #batches = [_ for _ in utils.gen_batches(train_set, self.batch_size)]
        updates = [self.W_CH_upd8, self.W_RH_upd8, self.L_upd8, self.b_h_upd8, self.b_c_upd8, self.b_r_upd8]


        total_loss_concepts = 0.0
        total_loss_relations = 0.0
        total_loss_affordances = 0.0
        total_acc_concepts = 0.0
        total_acc_relations = 0.0
        total_acc_affordances = 0.0

        for batch_name in batches_names:
            #batch = np.loadtxt(path_to_batches+batch_name)
         
            batch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T
            
            
            #self.L = tf.matrix_set_diag(self.L, tf.zeros([1,self.num_concepts]))
            self.tf_session.run(updates, feed_dict=self._create_feed_dict(batch))
            #self.L = tf.matrix_set_diag(self.L, tf.zeros([1,self.num_concepts]))
            #print batch_name+' is computed.'
            total_loss_concepts += self.tf_session.run(self.loss_concepts,feed_dict=self._create_feed_dict(batch))
            total_loss_relations += self.tf_session.run(self.loss_relations,feed_dict=self._create_feed_dict(batch))
            total_loss_affordances += self.tf_session.run(self.loss_affordances,feed_dict=self._create_feed_dict(batch))
            total_acc_concepts += self.tf_session.run(self.acc_concepts,feed_dict=self._create_feed_dict(batch))
            total_acc_relations += self.tf_session.run(self.acc_relations,feed_dict=self._create_feed_dict(batch))
            total_acc_affordances += self.tf_session.run(self.acc_affordances,feed_dict=self._create_feed_dict(batch))
        c_loss, r_loss, a_loss =  total_loss_concepts/len(batches_names), total_loss_relations/len(batches_names), total_loss_affordances/len(batches_names) 
        c_acc, r_acc, a_acc = total_acc_concepts/len(batches_names), total_acc_relations/len(batches_names), total_acc_affordances/len(batches_names) 
        print "train: c loss:"+str(c_loss)+' c acc:'+str(c_acc)+' r loss:'+str(r_loss)+' r acc:'+str(r_acc) +' a loss:'+str(a_loss)+' a acc:'+str(a_acc)
        self.train_history.append([c_loss,c_acc,r_loss,r_acc, a_loss, a_acc])
        print self.train_history
    def _create_feed_dict(self, data):

        """ Create the dictionary of data to feed to TensorFlow's session during training.
        :param data: training/validation set batch
        :return: dictionary(self.input_data: data, self.hrand: random_uniform, self.vrand: random_uniform)
        """

        return {
            self.input_data: data,
            self.hrand: np.random.rand(data.shape[0], self.num_hidden),
            self.crand: np.random.rand(data.shape[0], self.num_concepts),
            self.rrand: np.random.rand(data.shape[0], self.num_relations, self.num_concepts, self.num_concepts)
        }

    def fit(self, path_to_batches, path_to_validation_set=None, restore_previous_model=None):

        """ Fit the model to the training data.
        :param train_set: training set
        :param validation_set: validation set. optional, default None
        :param restore_previous_model:
                    if true, a previous trained model
                    with the same name of this model is restored from disk to continue training.
        :return: self
        """



        if path_to_validation_set is not None:
            print 'Validation set directory: '+path_to_validation_set

        self._build_model()

        with tf.Session() as self.tf_session:

            self._initialize_tf_utilities_and_ops(restore_previous_model)
            self._train_model(path_to_batches, path_to_validation_set)
    







    def _run_validation_error_and_summaries(self, epoch, path_to_batches):

        """ Run the summaries and error computation on the validation set.
        :param epoch: current epoch
        :param validation_set: validation data
        :return: self
        """


        batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]

        shuffle(batches_names)
 
           

        total_loss_concepts = 0.0
        total_loss_relations = 0.0
        total_loss_affordances = 0.0
        total_acc_concepts = 0.0
        total_acc_relations = 0.0
        total_acc_affordances = 0.0

        for batch_name in batches_names:
            #batch = np.loadtxt(path_to_batches+batch_name)
         
            batch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T

            total_loss_concepts += self.tf_session.run(self.loss_concepts,feed_dict=self._create_feed_dict(batch))
            total_loss_relations += self.tf_session.run(self.loss_relations,feed_dict=self._create_feed_dict(batch))
            total_loss_affordances += self.tf_session.run(self.loss_affordances,feed_dict=self._create_feed_dict(batch))
            total_acc_concepts += self.tf_session.run(self.acc_concepts,feed_dict=self._create_feed_dict(batch))
            total_acc_relations += self.tf_session.run(self.acc_relations,feed_dict=self._create_feed_dict(batch))
            total_acc_affordances += self.tf_session.run(self.acc_affordances,feed_dict=self._create_feed_dict(batch))
        c_loss, r_loss, a_loss =  total_loss_concepts/len(batches_names), total_loss_relations/len(batches_names), total_loss_affordances/len(batches_names) 
        c_acc, r_acc, a_acc = total_acc_concepts/len(batches_names), total_acc_relations/len(batches_names), total_acc_affordances/len(batches_names) 
        print "val: c loss:"+str(c_loss)+' c acc:'+str(c_acc)+' r loss:'+str(r_loss)+' r acc:'+str(r_acc) +' a loss:'+str(a_loss)+' a acc:'+str(a_acc)
        self.validation_history.append([c_loss,c_acc,r_loss,r_acc, a_loss, a_acc])
        print self.validation_history

        
        
    def task1SpatialRelationEstimation(self, path_to_batches, path_to_model, amplify):

        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')
        mask       = tf.placeholder('float', [self.batch_size, num_units], name='mask')
        
        concepts = input_data[:,0:self.num_concepts]
        relations = input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        # Create mask matrix.
        M = np.zeros((self.batch_size, num_units))
        M[:,:70] = 1
        M[:,70+70*70*0:70+70*70*1] = 1
        M[:,70+70*70*2:70+70*70*3] = 1
        
        # Reshape mask for concepts and relations.    
        maskC = tf.convert_to_tensor(M[:,0:self.num_concepts],dtype=tf.float32)
        maskR = tf.convert_to_tensor(M[:,self.num_concepts:],dtype=tf.float32)
        maskR = tf.reshape(maskR, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts])

        # Build model.
        self._build_model()
        

        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(1):
            #Concept'leri daima fixledik su an.

            hprobs, hstates = self.sample_hidden_from_visible((cstates)*amplify, (rprobs*maskR))

            #hprobs kullandik.
            _, _, rprobs, rstates = self.sample_visible_from_hidden_1(hstates, cstates, rstates, maskC, maskR)
        
        acc_relations = tf.reduce_mean(tf.cast(tf.equal(relations, rstates),tf.float64))
        acc_concepts = tf.reduce_mean(tf.cast(tf.equal(concepts, cstates),tf.float64)) 
        
        ori = tf.concat([concepts, tf.reshape(relations,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)
        rec = tf.concat([concepts, tf.reshape(rstates,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)

        probs = tf.concat([cprobs, tf.reshape(rprobs,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)
        
        
        #print 'TASK1: Spatial Relation Estimation:'
        #print 'modelname: ', self.model_name
        #print 'modelpath: ', path_to_model
        #print 'datapath : ', path_to_batches
        #print "amplify: ", amplify
        
        gpu_options = tf.GPUOptions(allow_growth=True)
        
        self.tf_session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))

   
        self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  

        batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]
        #print 'Total ',len(batches_names), ' batches are found.'
        
        acc1 = 0
        acc2 = 0
        acc3 = 0
        acc4 = 0
        
        for batch_name in batches_names:
            # Read batch and organize concepts and relations
            batch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T
            #print batch_name


            
            val0, val1 = self.tf_session.run([acc_concepts,acc_relations], feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val2 = self.tf_session.run(tf.reduce_sum(relations[:,1])+tf.reduce_sum(relations[:,3]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val3 = self.tf_session.run(tf.reduce_sum(rstates[:,1])+tf.reduce_sum(rstates[:,3]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val4 = self.tf_session.run(tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(rstates[:,1],'bool'),tf.cast(relations[:,1],'bool') ),'float'))+tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(rstates[:,3],'bool'),tf.cast(relations[:,3],'bool') ),'float')) ,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            #print "acc: concepts - relations: ", val1

            #print "number of 1s in original relations:", val2

            #print "number of 1s in reconstructed rstates:", val3

            #print "compare 1s of original rstates and rstates after reconstruction:", val4
            
            acc1 += val1
            acc2 += val2
            acc3 += val3
            acc4 += val4
            
            '''
            ori2 = self.tf_session.run(ori,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            rec2 = self.tf_session.run(rec,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            probs2 = self.tf_session.run(probs,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            hiddens2 = self.tf_session.run(hprobs,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            np.save(batch_name+'_task1_original',ori2.T)
            np.save(batch_name+'_task1_recovered',rec2.T)
            np.save(batch_name+'_task1_visible_probs',probs2.T)
            np.save(batch_name+'_task1_hiddens', hiddens2)
            '''
        
        print
        print path_to_model
        print acc1/len(batches_names)
        print acc2/len(batches_names)
        print acc3/len(batches_names)
        print acc4/len(batches_names)
        
        self.tf_session.close()
        #print 'DONE'
        
    def task4AffordanceEstimation(self, path_to_batches, path_to_model, amplify):

        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')
        mask       = tf.placeholder('float', [self.batch_size, num_units], name='mask')
        
        concepts = input_data[:,0:self.num_concepts]
        relations = input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        # Create mask matrix.
        M = np.zeros((self.batch_size, num_units))
        M[:,:70] = 1
        M[:,70+70*70*1:70+70*70*2] = 1
        M[:,70+70*70*3:70+70*70*4] = 1
        
        # Reshape mask for concepts and relations.    
        maskC = tf.convert_to_tensor(M[:,0:self.num_concepts],dtype=tf.float32)
        maskR = tf.convert_to_tensor(M[:,self.num_concepts:],dtype=tf.float32)
        maskR = tf.reshape(maskR, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts])

        # Build model.
        self._build_model()
        

        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(1):
            #Concept'leri daima fixledik su an.

            hprobs, hstates = self.sample_hidden_from_visible((cstates)*amplify, (rprobs*maskR))

            #hprobs kullandik.
            _, _, rprobs, rstates = self.sample_visible_from_hidden_1(hstates, cstates, rstates, maskC, maskR)
        
        acc_relations = tf.reduce_mean(tf.cast(tf.equal(relations, rstates),tf.float64))
        acc_concepts = tf.reduce_mean(tf.cast(tf.equal(concepts, cstates),tf.float64)) 
        
        ori = tf.concat([concepts, tf.reshape(relations,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)
        rec = tf.concat([concepts, tf.reshape(rstates,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)

        probs = tf.concat([cprobs, tf.reshape(rprobs,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)
        
        
        #print 'TASK4: Affordance Estimation:'
        #print 'modelname: ', self.model_name
        #print 'modelpath: ', path_to_model
        #print 'datapath : ', path_to_batches
        #print "amplify: ", amplify
        
        gpu_options = tf.GPUOptions(allow_growth=True)
        
        self.tf_session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))

   
        self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  

        batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]
        #print 'Total ',len(batches_names), ' batches are found.'
        
        acc1 = 0
        acc2 = 0
        acc3 = 0
        acc4 = 0
        
        for batch_name in batches_names:
            # Read batch and organize concepts and relations
            batch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T
            #print batch_name


            
            val0, val1 = self.tf_session.run([acc_concepts,acc_relations], feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val2 = self.tf_session.run(tf.reduce_sum(relations[:,0])+tf.reduce_sum(relations[:,2]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val3 = self.tf_session.run(tf.reduce_sum(rstates[:,0])+tf.reduce_sum(rstates[:,2]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val4 = self.tf_session.run(tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(rstates[:,0],'bool'),tf.cast(relations[:,0],'bool') ),'float'))+tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(rstates[:,2],'bool'),tf.cast(relations[:,2],'bool') ),'float')) ,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            #print "acc: concepts - relations: ", val1

            #print "number of 1s in original relations:", val2

            #print "number of 1s in reconstructed rstates:", val3

            #print "compare 1s of original rstates and rstates after reconstruction:", val4
            
            acc1 += val1
            acc2 += val2
            acc3 += val3
            acc4 += val4
            
            '''
            ori2 = self.tf_session.run(ori,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            rec2 = self.tf_session.run(rec,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            probs2 = self.tf_session.run(probs,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            hiddens2 = self.tf_session.run(hprobs,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})

            np.save(batch_name+'_task1_original',ori2.T)
            np.save(batch_name+'_task1_recovered',rec2.T)
            np.save(batch_name+'_task1_visible_probs',probs2.T)
            np.save(batch_name+'_task1_hiddens', hiddens2)
            '''
        
        print
        print path_to_model
        print acc1/len(batches_names)
        print acc2/len(batches_names)
        print acc3/len(batches_names)
        print acc4/len(batches_names)
        
        self.tf_session.close()
        #print 'DONE'     
    
    def task2WhatIsMissingOnTheScene(self, path_to_batches, path_to_model):
        # Var olan concept'leri ve relation'lari sabit tut.
        
        
        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        # self.input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')

        # Build model.
        self._build_model()
        concepts = self.input_data[:,0:self.num_concepts]
        relations = self.input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(1):
            #Concept'leri daima fixledik su an.
        
            hprobs, hstates = self.sample_hidden_from_visible(cstates, rstates)
            cprobs, cstates, rprobs, rstates = self.sample_visible_from_hidden_2(hstates, cstates, rstates)
        
        
        acc_relations = tf.reduce_mean(tf.cast(tf.equal(relations, rstates),tf.float64))
        acc_concepts = tf.reduce_mean(tf.cast(tf.equal(concepts, cstates),tf.float64)) 
        
        
        print 'TASK2: What is missin on the scene:'
        print 'modelname: ', self.model_name
        print 'modelpath: ', path_to_model
        print 'datapath : ', path_to_batches
        
        ori = tf.concat([concepts, tf.reshape(relations,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)
        rec = tf.concat([cstates, tf.reshape(rstates,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)

        probs = tf.concat([cprobs, tf.reshape(rprobs,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)
        
        
        acc1=0.0
        acc2=0.0
        acc3=0.0
        acc4=0.0

        with tf.Session() as self.tf_session:
            self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  

            batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]
            print 'Total ',len(batches_names), ' batches are found.'
            z = np.zeros((90,))
            counter=0
            for batch_name in batches_names:
                # Read batch and organize concepts and relations
                oribatch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T
                
                '''
                z = np.zeros((128,))
                batch = np.copy(oribatch)
                for i in range(batch.shape[0]):
                    for j in range(self.num_concepts):
                        if batch[i,j]==1:
                            batch[i,j] = 0
                            z[i] = j
                            break
                '''
                
                print counter
                counter+=1
                
                TP = 0.
                FP = 0.
                TN = 0.
                FN = 0.
                for masked_index in range(70):

                    batch = np.copy(oribatch)
                    #masked_index = 60 
                    batch[:,masked_index]=0
                    #print batch_name

                    nprec = self.tf_session.run(rec,   feed_dict=self._create_feed_dict(batch))

                    s = 0.0
                    n = 0.0

                    for i in range(128):
                        if oribatch[i,masked_index]==1: # there is missing object
                            if (nprec[i,masked_index]==1): # missing object is found.
                                TP += 1
                            elif (nprec[i,masked_index]==0): # missing object not found.
                                FN += 1
                                
                        elif oribatch[i,masked_index]==0: # there is no missing object
                            if (nprec[i,masked_index]==1): # missing object is found.
                                FP += 1
                            elif (nprec[i,masked_index]==0):
                                TN += 1
                            
                            

                print TP, FP, TN, FN
                precision = TP /(TP+FP)
                recall = TP / (TP+FN)
                f1 = 2 / ((1/recall)+(1/precision))
                
                print precision, recall, f1
                
                print 'precision'

                val0, val1 = self.tf_session.run([acc_concepts,acc_relations], feed_dict=self._create_feed_dict(batch))

                #val2 = self.tf_session.run(tf.reduce_sum(tf.convert_to_tensor(oribatch[:,:70])),   feed_dict=self._create_feed_dict(batch))
                val2 = np.sum(oribatch[:,:70])
                
                val3 = self.tf_session.run(tf.reduce_sum(cstates),   feed_dict=self._create_feed_dict(batch))

                val4 = self.tf_session.run(tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(cstates,'bool'),tf.cast(tf.convert_to_tensor(oribatch[:,:70]),'bool') ),tf.int32)),   feed_dict=self._create_feed_dict(batch))
                acc1+=val1
                acc2+=val2
                acc3+=val3
                acc4+=val4


                

                
                
                
                
            print path_to_model
            print acc1/len(batches_names)
            print acc2/len(batches_names)
            print acc3/len(batches_names)
            print acc4/len(batches_names)
            print




            print z/len(batches_names)
        self.tf_session.close()
        print 'DONE'  
    
    def additionalTask(self, path_to_model):
        # Var olan concept'leri ve relation'lari sabit tut.
        
        
        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        # self.input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')

        # Build model.
        self._build_model()
        concepts = self.input_data[:,0:self.num_concepts]
        relations = self.input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(1):
            #Concept'leri daima fixledik su an.
        
            hprobs, hstates = self.sample_hidden_from_visible(cstates, rstates)
            cprobs, cstates, rprobs, rstates = self.sample_visible_from_hidden_3(hstates, cstates, rstates)        




        with tf.Session() as self.tf_session:
            self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  

            '''
            # Read batch and organize concepts and relations
            batch = np.zeros((1,417+ 4*417*417))
            batch[0, 165] = 1 # fridge
            batch[0, 241] = 1 # microwave
            #batch[0, 118] = 1
            #batch[0, 381] = 1 # toilet
            
            

            #print self.tf_session.run(concepts,   feed_dict=self._create_feed_dict(batch))
            rec = self.tf_session.run(cprobs,   feed_dict=self._create_feed_dict(batch))
            #print rec
            
            print np.argsort(rec)[0, 410:]
            
            print rec[0, np.argsort(rec)[0, 410:] ]
            '''

            # Read batch and organize concepts and relations
            batch = np.zeros((1,417+ 4*417*417))
            batch[0, 367] = 1 # fridge
            batch[0, 250] = 1 # microwave
            batch[0, 67] = 1
            batch[0, 396] = 1 # toilet
            print batch
            print np.where(batch==1)
            rec = self.tf_session.run(cprobs,   feed_dict=self._create_feed_dict(batch))
            print np.where(rec==1)
            print rec
            
        self.tf_session.close()
        print 'DONE'  
    
    
    
    def additionalTask_2(self, path_to_model):
        # Var olan concept'leri ve relation'lari sabit tut.
        
        
        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        # self.input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')

        # Build model.
        self._build_model()
        concepts = self.input_data[:,0:self.num_concepts]
        relations = self.input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(2):
            #Concept'leri daima fixledik su an.
        
            hprobs, hstates = self.sample_hidden_from_visible(cstates, rstates)
            cprobs, cstates, rprobs, rstates = self.sample_visible_from_hidden(hstates*2, cstates, rstates)        




        with tf.Session() as self.tf_session:
            self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  
            
            M1 = np.loadtxt('samples_maskrcnn_txt_got_from_NYU_with_90vocab', delimiter=',')
            M2 = np.loadtxt('ground_truth_got_from_NYU_with_90vocab', delimiter=',')

            M1 = np.hstack((M1, np.zeros((97, 90*90*14))))
            M2 = np.hstack((M2, np.zeros((97, 90*90*14))))
            
            np_cstates = self.tf_session.run(cstates,   self._create_feed_dict(M1))    
            
            print np_cstates.shape
            print np.sum(M2[:,:90])
            print
            
            print np.sum(M1[:,:90]==M2[:,:90])
            print np.sum(np_cstates[:,:90]==M2[:,:90])
            print
            
            print np.sum(M1[:,:90])
            print 'TP', np.sum(np.logical_and(M1[:,:90], M2[:,:90]))
            
            print
            print np.sum(np_cstates[:,:90])
            
            G_P = np.sum(M2[:,:90])
            
            TP_detector = np.sum(np.logical_and(M1[:,:90], M2[:,:90]))
            P_detector  = np.sum(M1[:,:90])
            
            
            
            TP_model = np.sum(np.logical_and(np_cstates[:,:90], M2[:,:90]))
            P_model  = np.sum(np_cstates[:,:90])            
            
            print 'P_ground', G_P
            print
            print 'P_detector', P_detector
            print 'TP_detector', TP_detector
            print
            print 'P_model', P_model
            print 'TP_model', TP_model
            
            print

            print 'map detector ', TP_detector / P_detector
            print 'map model ', TP_model / P_model
            #print 'G_P', G_P
            
            
        self.tf_session.close()
        print 'DONE'              
        
        
    
    
    def task3WhatIsExtraOnTheScene(self, path_to_batches, path_to_model):
        # Var olan concept'leri ve relation'lari sabit tut.
        
        
        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        # self.input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')
        
        
        

        # Build model.
        self._build_model()
        concepts = self.input_data[:,0:self.num_concepts]
        relations = self.input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(1):
            #Concept'leri daima fixledik su an.
        
            hprobs, hstates = self.sample_hidden_from_visible(cstates, rstates)
            cprobs, cstates, rprobs, rstates = self.sample_visible_from_hidden_3(hstates, cstates, rstates)
        
        ori = tf.concat([concepts, tf.reshape(relations,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)
        rec = tf.concat([cstates, tf.reshape(rstates,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)

        probs = tf.concat([cprobs, tf.reshape(rprobs,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)
        acc_relations = tf.reduce_mean(tf.cast(tf.equal(relations, rstates),tf.float64))
        acc_concepts = tf.reduce_mean(tf.cast(tf.equal(concepts, cstates),tf.float64)) 
        
        print 'TASK3: What is extra on the sceen?:'
        print 'modelname: ', self.model_name
        print 'modelpath: ', path_to_model
        print 'datapath : ', path_to_batches
        
        acc1=0.0
        acc2=0.0
        acc3=0.0
        acc4=0.0

        TP = 0.0 # 0 olmasi gereken konseptler
        FP = 0.0 # 0 yapilan ama aslinda 1 olmasi gereken konseptler
        TN = 0.0 # 1 yapilan ve 1 olmasi gereken konseptler
        FN = 0.0 # 1 yapilan ama 0 olmasi gereken konseptler
        
        with tf.Session() as self.tf_session:
            self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  

            batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]
            print 'Total ',len(batches_names), ' batches are found.'
            z = np.zeros((70,))
            counter=0
            dist = np.zeros((70,))
            for batch_name in batches_names:
                # Read batch and organize concepts and relations
                oribatch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T
                
                '''
                z = np.zeros((128,))
                batch = np.copy(oribatch)
                for i in range(batch.shape[0]):
                    for j in range(self.num_concepts):
                        if batch[i,j]==1:
                            batch[i,j] = 0
                            z[i] = j
                            break
                '''
                
                print counter
                counter+=1

                print dist
                for masked_index in range(70):
                    batch=np.copy(oribatch)


                    batch[:, masked_index] = 1

                    nprec = self.tf_session.run(rec,   feed_dict=self._create_feed_dict(batch))

                    dist[masked_index] += np.sum(nprec[:,:70]!=oribatch[:,:70])/float(np.sum(oribatch[:,:70]==1))

                ''''
                for masked_index in range(70):
		
                    batch = np.copy(oribatch)
                    #masked_index = 60 
                    batch[:,masked_index]=1
                    #print batch_name

                    #Reconstructed masked batch.
                    nprec = self.tf_session.run(rec,   feed_dict=self._create_feed_dict(batch))


                    for i in range(128):
                        # Orijinal batch'de 0 ise ama batch'de aktif yapildiysa.
                        if oribatch[i,masked_index] == 0 and nprec[i,masked_index]==0:
                            TP += 1
                            
                        elif oribatch[i,masked_index] == 1 and nprec[i,masked_index]==0:
                            FP += 1
                
                        elif oribatch[i,masked_index] == 1 and nprec[i,masked_index]==1:
                            TN += 1
                        
                        elif oribatch[i,masked_index] == 0 and nprec[i,masked_index]==1:
                            FN += 1


                #val0, val1 = self.tf_session.run([acc_concepts,acc_relations], feed_dict=self._create_feed_dict(batch))

                #val2 = self.tf_session.run(tf.reduce_sum(tf.convert_to_tensor(oribatch[:,:70])),   feed_dict=self._create_feed_dict(batch))
                val2 = np.sum(oribatch[:,:70]==0)
                
                #val3 = self.tf_session.run(tf.reduce_sum(cstates),   feed_dict=self._create_feed_dict(batch))

                #val4 = self.tf_session.run(tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(cstates,'bool'),tf.cast(tf.convert_to_tensor(oribatch[:,:70]),'bool') ),tf.int32)),   feed_dict=self._create_feed_dict(batch))
                #acc1+=val1
                acc2+=val2
                #acc3+=val3
                #acc4+=val4


            print path_to_model
            print acc1/len(batches_names)
            print acc2/len(batches_names)
            print acc3/len(batches_names)
            print acc4/len(batches_names)
            print

            print TP/len(batches_names)
            print FP/len(batches_names)
            print TN/len(batches_names)
            print FN/len(batches_names)
	    '''
	    print dist/(len(batches_names))

        self.tf_session.close()
        print 'DONE'  
    
    
    
    
    
    
    
    
    def load_model(self, shape, gibbs_sampling_steps, model_path):

        """ Load a trained model from disk. The shape of the model
        (num_visible, num_hidden) and the number of gibbs sampling steps
        must be known in order to restore the model.
        :param shape: tuple(num_concept,num_relation, num_hidden)
        :param gibbs_sampling_steps:
        :param model_path:
        :return: self
        """

        self.num_concepts,self.num_relations, self.num_hidden = shape[0], shape[1], shape[2]
        self.gibbs_sampling_steps = gibbs_sampling_steps

        self._build_model()

        init_op = tf.global_variables_initializer()
        self.tf_saver = tf.train.Saver()

        with tf.Session() as self.tf_session:

            self.tf_session.run(init_op)
            self.tf_saver.restore(self.tf_session, model_path)


    def get_model_parameters(self, model_path=None):

        """ Return the model parameters in the form of numpy arrays.
        :return: model parameters
        """
        if model_path==None:
            model_path=self.model_path
   
        with tf.Session() as self.tf_session:

            self.tf_saver.restore(self.tf_session, model_path)

            return {
                'W_CH': self.W_CH.eval(),
                'W_RH': self.W_RH.eval(),
                'L': self.L.eval(),
                'b_h': self.b_h.eval(),
                'b_c': self.b_c.eval(),
                'b_r': self.b_r.eval()
            }
    def sample_visible_from_hidden_1(self, hstates, cstates, rstates, maskC, maskR):

        """ Sample the visible units from the hidden units.
        This is the Negative phase of the Contrastive Divergence algorithm.
        :param hidden: activations of the hidden units, concept units and relation units
        :return: visible probabilities and states.
        """
        # Sample visible for Task 1: Relation estimation.
        # Concepts are fixed. Relations are sampled.
        
        
        c = tf.concat([cstates]*self.num_relations*self.num_concepts,1)
        L_expanded = tf.expand_dims(self.L,0)
        L_expanded = tf.concat([L_expanded]*self.batch_size,0,name='L_expanded')

        # TODO relation'larin activation'ini kontrol et.
        w_rh = tf.reshape(self.W_RH,[self.num_relations*self.num_concepts*self.num_concepts, self.num_hidden])
        cc   = tf.reshape( tf.concat([cstates]*self.num_concepts,1),[-1,self.num_concepts,self.num_concepts])
        cc_prod = cc*tf.transpose(cc,[0,2,1])
        cc_prod = tf.concat([tf.reshape(cc_prod,[-1,self.num_concepts*self.num_concepts])]*self.num_relations,axis=1,name='cc_prod')
        cc_prod_reshaped = tf.reshape(cc_prod,[-1,self.num_relations, self.num_concepts, self.num_concepts],name='cc_prod_reshaped') #Yanlislik olabilir mi?


        act1 = tf.matmul(hstates,tf.transpose(w_rh))
        act1_reshaped = tf.reshape(act1, [-1,self.num_relations,self.num_concepts,self.num_concepts])
        act2 = cc_prod_reshaped*L_expanded

        r_activation = act1_reshaped+act2 + self.b_r
        r_probs = tf.nn.sigmoid(r_activation)
        r_units = utils.sample_prob(r_probs,self.rrand)
        
        
        
        #Added
        r_units = rstates*maskR+r_units*(1-maskR)
        r_probs = rstates*maskR+r_probs*(1-maskR)
        
        
        
        return cstates, cstates,r_probs, r_units
    
    
    
    
    
    def sample_visible_from_hidden_2(self, hstates, cstates, rstates):

        """ Sample the visible units from the hidden units.
        This is the Negative phase of the Contrastive Divergence algorithm.
        :param hidden: activations of the hidden units, concept units and relation units
        :return: visible probabilities and states.
        """
        # Sample visible for Task 2: What is missing on the sceen?
        # Active concepts are fixed, Sample inactive(0 valued) concepts.  Relations are fixed.
        ori = cstates

        c = tf.concat([cstates]*self.num_relations*self.num_concepts,1)
        L_expanded = tf.expand_dims(self.L,0)
        L_expanded = tf.concat([L_expanded]*self.batch_size,0,name='L_expanded')

        
        # Sample Concepts from Concepts, Relations and Hidden Units
        # Create a=[r11c1,r12c2,r13,c3,..,r21c1,r22c2,r23,c3,...]
        r = tf.reshape(rstates,[-1, self.num_relations*self.num_concepts*self.num_concepts])
        a = r*c
        a.set_shape((self.batch_size, a.get_shape()[1]))

        
        
        # Create a matrix [[L11r11c1,L12r12c2,L13r13c3,..],[L21r21c1,L22r22c2,L23r23c3,..],...]
        a = tf.reshape(a,[self.batch_size, self.num_relations,self.num_concepts,self.num_concepts])
        temp = tf.reduce_sum(a*L_expanded, axis=1)
        s_act  = tf.reduce_sum(temp, axis=2)



        # Create a matrix [[L11r11c1,L12r12c1,L13r13c1,..],[L21r21c2,L22r22c2,L23r23c2,..],...]
        # KONTROL ET, TRANSPOSE ALINCA SORUN COZULDU MU YANI? MUHTEMELEN COZULDU.
        r = tf.transpose(rstates, [0,1,3,2])
        r = tf.reshape(rstates,[-1, self.num_relations*self.num_concepts*self.num_concepts])
        a = r*c
        a.set_shape((self.batch_size, a.get_shape()[1]))
        a = tf.reshape(a,[self.batch_size, self.num_relations,self.num_concepts,self.num_concepts])

        temp = tf.reduce_sum(a*L_expanded, axis=1)
        o_act  = tf.reduce_sum(temp, axis=2)


        def cond(cnt, cunits,cprobs):
            return tf.less(cnt,self.num_concepts)

        def body(cnt, cunits,cprobs):

            c_activation = tf.matmul(hstates, tf.transpose(self.W_CH)) + (s_act + o_act) + self.b_c
            c_probs = tf.nn.sigmoid(c_activation)

            new = utils.sample_prob(c_probs, self.crand)
            new_row = tf.gather(tf.transpose(new), [cnt])


            first_part = tf.gather(tf.transpose(cunits), tf.range(0,cnt) )
            second_part = tf.gather(tf.transpose(cunits), tf.range(cnt+1, self.num_concepts) )

            c_units = tf.transpose(tf.concat([first_part, new_row, second_part], 0))
            c_units.set_shape((self.batch_size,self.num_concepts))


            first_part_prob = tf.gather(tf.transpose(cprobs), tf.range(0,cnt) )
            second_part_prob = tf.gather(tf.transpose(cprobs), tf.range(cnt+1, self.num_concepts) )
            new_row_prob = tf.gather(tf.transpose(c_probs), [cnt])

            c_probs = tf.transpose(tf.concat([first_part_prob, new_row_prob, second_part_prob], 0))
            c_probs.set_shape((self.batch_size,self.num_concepts))

       
            c_units = tf.cast(tf.logical_or(tf.cast(ori,'bool'), tf.cast(c_units,'bool')),'float')
            c_probs = tf.clip_by_value(ori+c_probs, clip_value_min=0.,clip_value_max=1.0)
            
            return cnt+1, c_units,c_probs

        _, c_units, c_probs = tf.while_loop(cond, body, [tf.constant(0), cstates, cstates])

        return c_probs,c_units ,rstates, rstates
    
    
    
    def sample_visible_from_hidden_3(self, hstates, cstates, rstates):

        """ Sample the visible units from the hidden units.
        This is the Negative phase of the Contrastive Divergence algorithm.
        :param hidden: activations of the hidden units, concept units and relation units
        :return: visible probabilities and states.
        """
        # Sample visible for Task 3: What is extra on the sceen?
        # Inactive concepts are fixed, Sample active(1 valued) concepts.  Relations are fixed.
        ori = cstates

        c = tf.concat([cstates]*self.num_relations*self.num_concepts,1)
        L_expanded = tf.expand_dims(self.L,0)
        L_expanded = tf.concat([L_expanded]*self.batch_size,0,name='L_expanded')

        
        # Sample Concepts from Concepts, Relations and Hidden Units
        # Create a=[r11c1,r12c2,r13,c3,..,r21c1,r22c2,r23,c3,...]
        r = tf.reshape(rstates,[-1, self.num_relations*self.num_concepts*self.num_concepts])
        a = r*c
        a.set_shape((self.batch_size, a.get_shape()[1]))

        
        
        # Create a matrix [[L11r11c1,L12r12c2,L13r13c3,..],[L21r21c1,L22r22c2,L23r23c3,..],...]
        a = tf.reshape(a,[self.batch_size, self.num_relations,self.num_concepts,self.num_concepts])
        temp = tf.reduce_sum(a*L_expanded, axis=1)
        s_act  = tf.reduce_sum(temp, axis=2)



        # Create a matrix [[L11r11c1,L12r12c1,L13r13c1,..],[L21r21c2,L22r22c2,L23r23c2,..],...]
        # KONTROL ET, TRANSPOSE ALINCA SORUN COZULDU MU YANI? MUHTEMELEN COZULDU.
        r = tf.transpose(rstates, [0,1,3,2])
        r = tf.reshape(rstates,[-1, self.num_relations*self.num_concepts*self.num_concepts])
        a = r*c
        a.set_shape((self.batch_size, a.get_shape()[1]))
        a = tf.reshape(a,[self.batch_size, self.num_relations,self.num_concepts,self.num_concepts])

        temp = tf.reduce_sum(a*L_expanded, axis=1)
        o_act  = tf.reduce_sum(temp, axis=2)


        def cond(cnt, cunits,cprobs):
            return tf.less(cnt,self.num_concepts)

        def body(cnt, cunits,cprobs):

            c_activation = tf.matmul(hstates, tf.transpose(self.W_CH)) + (s_act + o_act) + self.b_c
            c_probs = tf.nn.sigmoid(c_activation)

            new = utils.sample_prob(c_probs, self.crand)
            new_row = tf.gather(tf.transpose(new), [cnt])


            first_part = tf.gather(tf.transpose(cunits), tf.range(0,cnt) )
            second_part = tf.gather(tf.transpose(cunits), tf.range(cnt+1, self.num_concepts) )

            c_units = tf.transpose(tf.concat([first_part, new_row, second_part], 0))
            c_units.set_shape((self.batch_size,self.num_concepts))


            first_part_prob = tf.gather(tf.transpose(cprobs), tf.range(0,cnt) )
            second_part_prob = tf.gather(tf.transpose(cprobs), tf.range(cnt+1, self.num_concepts) )
            new_row_prob = tf.gather(tf.transpose(c_probs), [cnt])

            c_probs = tf.transpose(tf.concat([first_part_prob, new_row_prob, second_part_prob], 0))
            c_probs.set_shape((self.batch_size,self.num_concepts))

            # key point
            # TODO
            c_units = c_units*ori
            c_probs = c_probs*ori
            
            return cnt+1, c_units,c_probs

        _, c_units, c_probs = tf.while_loop(cond, body, [tf.constant(0), cstates, cstates])

        return c_probs,c_units ,rstates, rstates    
    
    
    
    def task5WhatisTheObjectForAffordances(self, path_to_batches, path_to_model, amplify):

        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')
        mask       = tf.placeholder('float', [self.batch_size, num_units], name='mask')
        
        concepts = input_data[:,0:self.num_concepts]
        relations = input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        # Create mask matrix.
        M = np.zeros((self.batch_size, num_units))
        M[:,:70] = 1
        M[:,70+70*70*0:70+70*70*1] = 1
        M[:,70+70*70*1:70+70*70*2] = 1
        M[:,70+70*70*3:70+70*70*4] = 1
        
        # Reshape mask for concepts and relations.    
        maskC = tf.convert_to_tensor(M[:,0:self.num_concepts],dtype=tf.float32)
        maskR = tf.convert_to_tensor(M[:,self.num_concepts:],dtype=tf.float32)
        maskR = tf.reshape(maskR, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts])

        # Build model.
        self._build_model()
        

        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(1):
            #Concept'leri daima fixledik su an.

            hprobs, hstates = self.sample_hidden_from_visible((cstates)*amplify, (rprobs*maskR))

            #hprobs kullandik.
            _, _, rprobs, rstates = self.sample_visible_from_hidden_1(hstates, cstates, rstates, maskC, maskR)
        
        acc_relations = tf.reduce_mean(tf.cast(tf.equal(relations, rstates),tf.float64))
        acc_concepts = tf.reduce_mean(tf.cast(tf.equal(concepts, cstates),tf.float64)) 
        
        ori = tf.concat([concepts, tf.reshape(relations,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)
        rec = tf.concat([concepts, tf.reshape(rstates,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)

        probs = tf.concat([cprobs, tf.reshape(rprobs,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)
        
        
        #print 'TASK4: Affordance Estimation:'
        #print 'modelname: ', self.model_name
        #print 'modelpath: ', path_to_model
        #print 'datapath : ', path_to_batches
        #print "amplify: ", amplify
        
        gpu_options = tf.GPUOptions(allow_growth=True)
        
        self.tf_session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))

   
        self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  

        batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]
        #print 'Total ',len(batches_names), ' batches are found.'
        
        acc1 = 0
        acc2 = 0
        acc3 = 0
        acc4 = 0
        
        counter = 0
        for batch_name in batches_names:
            # Read batch and organize concepts and relations
            batch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T
            #print batch_name
            # print counter
            # counter += 1
            
            np_relations = self.tf_session.run(relations[:,2,2:4],   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            np_rstates  = self.tf_session.run(rstates[:,2,2:4],   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            '''
            val0, val1 = self.tf_session.run([acc_concepts,acc_relations], feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val2 = self.tf_session.run(tf.reduce_sum(relations[:,2,2:4]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val3 = self.tf_session.run(tf.reduce_sum(rstates[:,2,2:4]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val4 = self.tf_session.run(tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(rstates[:,2,2:4],'bool'),tf.cast(relations[:,2,2:4],'bool') ),'float')) ,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            '''
            val2 = np.sum(np_relations)
            val3 = np.sum(np_rstates)
            val4 = np.sum(np.logical_and(np_relations, np_rstates))
            
            #acc1 += val1
            acc2 += val2
            acc3 += val3
            acc4 += val4
            

        
        print
        print path_to_model
        print acc1/len(batches_names)
        print acc2/len(batches_names)
        print acc3/len(batches_names)
        print acc4/len(batches_names)
        
        self.tf_session.close()
        #print 'DONE'     
        
        
        
    def task6WhatisTheSubjectForAffordances(self, path_to_batches, path_to_model, amplify):

        # Create placeholder for data and mask
        num_units = self.num_concepts+self.num_relations*self.num_concepts*self.num_concepts
        input_data = tf.placeholder('float', [self.batch_size, num_units], name='input_data')
        mask       = tf.placeholder('float', [self.batch_size, num_units], name='mask')
        
        concepts = input_data[:,0:self.num_concepts]
        relations = input_data[:,self.num_concepts:]
        relations = tf.reshape(relations, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts] )
        
        # Create mask matrix.
        M = np.zeros((self.batch_size, num_units))
        M[:,:70] = 1
        M[:,70+70*70*0:70+70*70*1] = 1
        M[:,70+70*70*1:70+70*70*2] = 1
        M[:,70+70*70*3:70+70*70*4] = 1
        
        # Reshape mask for concepts and relations.    
        maskC = tf.convert_to_tensor(M[:,0:self.num_concepts],dtype=tf.float32)
        maskR = tf.convert_to_tensor(M[:,self.num_concepts:],dtype=tf.float32)
        maskR = tf.reshape(maskR, [self.batch_size,self.num_relations,self.num_concepts,self.num_concepts])

        # Build model.
        self._build_model()
        

        
        cstates = concepts
        cprobs = concepts
        rstates = relations
        rprobs = relations
        for i in range(1):
            #Concept'leri daima fixledik su an.

            hprobs, hstates = self.sample_hidden_from_visible((cstates)*amplify, (rprobs*maskR))

            #hprobs kullandik.
            _, _, rprobs, rstates = self.sample_visible_from_hidden_1(hstates, cstates, rstates, maskC, maskR)
        
        acc_relations = tf.reduce_mean(tf.cast(tf.equal(relations, rstates),tf.float64))
        acc_concepts = tf.reduce_mean(tf.cast(tf.equal(concepts, cstates),tf.float64)) 
        
        ori = tf.concat([concepts, tf.reshape(relations,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])],1)
        rec = tf.concat([concepts, tf.reshape(rstates,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)

        probs = tf.concat([cprobs, tf.reshape(rprobs,[self.batch_size, self.num_relations*self.num_concepts*self.num_concepts])], 1)
        
        
        #print 'TASK4: Affordance Estimation:'
        #print 'modelname: ', self.model_name
        #print 'modelpath: ', path_to_model
        #print 'datapath : ', path_to_batches
        #print "amplify: ", amplify
        
        gpu_options = tf.GPUOptions(allow_growth=True)
        
        self.tf_session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))

   
        self._initialize_tf_utilities_and_ops(restore_path=path_to_model)  

        batches_names = [f for f in listdir(path_to_batches) if isfile(join(path_to_batches, f))]
        #print 'Total ',len(batches_names), ' batches are found.'
        
        acc1 = 0
        acc2 = 0
        acc3 = 0
        acc4 = 0
        
        counter = 0
        for batch_name in batches_names:
            # Read batch and organize concepts and relations
            batch = pandas.read_csv(path_to_batches+batch_name,sep=',',header=None,error_bad_lines=False).values.T
            #print batch_name
            # print counter
            # counter += 1
            
            np_relations = self.tf_session.run(relations[:,2, : , 4]+relations[:,2, : , 51],   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            np_rstates  = self.tf_session.run(rstates[:,2, : , 4]+rstates[:,2, : , 51],   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            '''
            val0, val1 = self.tf_session.run([acc_concepts,acc_relations], feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val2 = self.tf_session.run(tf.reduce_sum(relations[:,2,2:4]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val3 = self.tf_session.run(tf.reduce_sum(rstates[:,2,2:4]),   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            
            val4 = self.tf_session.run(tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(rstates[:,2,2:4],'bool'),tf.cast(relations[:,2,2:4],'bool') ),'float')) ,   feed_dict={input_data:batch, mask:M, self.hrand: np.random.rand(self.batch_size, self.num_hidden), self.rrand: np.random.rand(self.batch_size, self.num_relations, self.num_concepts, self.num_concepts)})
            '''
            val2 = np.sum(np_relations)
            val3 = np.sum(np_rstates)
            val4 = np.sum(np.logical_and(np_relations, np_rstates))
            
            #acc1 += val1
            acc2 += val2
            acc3 += val3
            acc4 += val4
            

        
        print
        print path_to_model
        print acc1/len(batches_names)
        print acc2/len(batches_names)
        print acc3/len(batches_names)
        print acc4/len(batches_names)
        
        self.tf_session.close()
        #print 'DONE'           
